{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe192b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Wide ETL Municipios (Madrid < 50k):\n",
    "- Google Places (Nearby Search): recuento + rating medio ponderado + total reseñas\n",
    "- Google Air Quality API: condición actual\n",
    "Salida: CSV wide (1 fila por municipio)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import logging\n",
    "from typing import Dict, List, Any, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a38434b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== CONFIGURACIÓN ===============\n",
    "load_dotenv()\n",
    "\n",
    "# --- Claves y rutas ---\n",
    "GOOGLE_PLACES_API_KEY = os.getenv(\"GOOGLE_PLACES_API_KEY\") or os.getenv(\"API_KEY\")\n",
    "GOOGLE_AIR_API_KEY    = os.getenv(\"GOOGLE_AIR_API_KEY\") or GOOGLE_PLACES_API_KEY\n",
    "\n",
    "INPUT_CSV  = os.path.join(\"../raw_data\", \"municipios_madrid_menores_50000.csv\")\n",
    "OUT_WIDE   = os.path.join(\"../raw_data\", \"municipios_google_data.csv\") # Nombre actualizado\n",
    "\n",
    "CACHE_DIR  = os.path.join(\"cache\")\n",
    "CACHE_PLACES_SEARCH  = os.path.join(CACHE_DIR, \"places_search\")\n",
    "CACHE_AIR            = os.path.join(CACHE_DIR, \"air_quality\")\n",
    "os.makedirs(CACHE_PLACES_SEARCH,  exist_ok=True)\n",
    "os.makedirs(CACHE_AIR,            exist_ok=True)\n",
    "os.makedirs(os.path.dirname(OUT_WIDE), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962646a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 02:11:43,750 [INFO] [1/155] Acebeda (La)\n",
      "2025-11-20 02:11:43,752 [INFO] Municipio: Acebeda (La) (14)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 383\u001b[39m\n\u001b[32m    380\u001b[39m     logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOK. CSV wide: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUT_WIDE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 374\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m df.iterrows():\n\u001b[32m    373\u001b[39m     logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[COL_NAME]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     rec = \u001b[43mprocess_municipio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_status\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m     rows.append(rec)\n\u001b[32m    377\u001b[39m     \u001b[38;5;66;03m# Guardado incremental (por si se interrumpe)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 328\u001b[39m, in \u001b[36mprocess_municipio\u001b[39m\u001b[34m(row, api_status)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cat_key, subtypes \u001b[38;5;129;01min\u001b[39;00m PLACE_TYPES.items():\n\u001b[32m    327\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m         metrics = \u001b[43mfetch_places_for_category\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m         rec.update(metrics)\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m GoogleAPICriticalError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 214\u001b[39m, in \u001b[36mfetch_places_for_category\u001b[39m\u001b[34m(lat, lon, cat_key, subtypes)\u001b[39m\n\u001b[32m    211\u001b[39m         _cache_write(cache_path, data)\n\u001b[32m    213\u001b[39m     all_results.extend(data)\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSLEEP_BETWEEN_REQS\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Evita saturar la API\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[38;5;66;03m# 2. Deduplicación (por place_id)\u001b[39;00m\n\u001b[32m    217\u001b[39m dedup = {}\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Parámetros generales ---\n",
    "RADIUS_METERS = 2000          # radio de consulta\n",
    "MAX_PAGES = 1                 # Nearby Search: 1 página (20 resultados) para contener cuota\n",
    "SLEEP_BETWEEN_REQS = 1.2      # segundos entre peticiones\n",
    "SLEEP_NEXT_PAGE = 2.2         # espera para next_page_token de Google\n",
    "TIMEOUT = 30                  # timeout HTTP\n",
    "MAX_RETRIES = 3               # Máximo de reintentos para errores transitorios (5xx)\n",
    "\n",
    "# --- Columnas esperadas en el CSV ---\n",
    "COL_ID    = \"cod_municipio\"\n",
    "COL_NAME  = \"municipio\"\n",
    "COL_LAT   = \"latitud\"\n",
    "COL_LON   = \"longitud\"\n",
    "COL_POP   = \"poblacion\"\n",
    "\n",
    "# --- Categorías Google (EXTENDIDAS) ---\n",
    "PLACE_TYPES: Dict[str, List[str]] = {\n",
    "    # 1. Servicios básicos y cotidianos\n",
    "    \"g_supermercados\": [\"supermarket\", \"grocery_or_supermarket\"],\n",
    "    \"g_conveniencia\": [\"convenience_store\"],\n",
    "    \"g_farmacias\": [\"pharmacy\"],\n",
    "    \"g_bancos\": [\"bank\"],\n",
    "    \"g_cajeros\": [\"atm\"],\n",
    "    \"g_gasolineras\": [\"gas_station\"],\n",
    "\n",
    "    # 2. Salud y asistencia\n",
    "    \"g_hospitales_clinicas\": [\"hospital\", \"clinic\"],\n",
    "    \"g_medicos_familia\": [\"doctor\"],\n",
    "    \n",
    "    # 3. Educación y formación\n",
    "    \"g_escuelas_infantiles\": [\"preschool\", \"kindergarten\"],\n",
    "    \"g_colegios_institutos\": [\"primary_school\", \"secondary_school\", \"school\"],\n",
    "    \"g_universidad_fp\": [\"university\", \"vocational_school\"],\n",
    "    \n",
    "    # 4. Transporte (Antes OSM - Mapeado a Google Place Types)\n",
    "    \"g_paradas_bus\": [\"bus_station\", \"bus_stop\"],\n",
    "    \"g_estaciones_principales\": [\"train_station\", \"subway_station\", \"transit_station\"],\n",
    "    \"g_aparcamientos\": [\"parking\"],\n",
    "\n",
    "    # 5. Ocio y cultura\n",
    "    \"g_restaurantes\": [\"restaurant\"],\n",
    "    \"g_cafeterias\": [\"cafe\"],\n",
    "    \"g_bares\": [\"bar\"],\n",
    "    \"g_cines\": [\"movie_theater\"],\n",
    "    \"g_gimnasios\": [\"gym\"],\n",
    "    \"g_parques\": [\"park\"], # Mapea parques (lo más parecido a entorno)\n",
    "    \n",
    "    # 6. Comercio\n",
    "    \"g_centros_comerciales\": [\"shopping_mall\"],\n",
    "    \n",
    "    # 7. Seguridad y administración (Antes OSM - Mapeado a Google Place Types)\n",
    "    \"g_comisarias\": [\"police\"],\n",
    "    \"g_bomberos\": [\"fire_station\"],\n",
    "    \"g_ayuntamientos\": [\"city_hall\"],\n",
    "    \"g_juzgados\": [\"court\"],\n",
    "    \n",
    "    # NOTA: Bosques y rutas de senderismo no tienen Place Type en Google. \n",
    "    # Solo \"park\" se mantiene como indicador de entorno natural.\n",
    "}\n",
    "\n",
    "\n",
    "# =============== LOGGING ===============\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n",
    "\n",
    "# =============== HELPERS ===============\n",
    "class GoogleAPIError(Exception):\n",
    "    \"\"\"Excepción base para errores de Google API.\"\"\"\n",
    "    pass\n",
    "\n",
    "class GoogleAPICriticalError(GoogleAPIError):\n",
    "    \"\"\"Error crítico (403, 401) que debe detener la ejecución de esa API.\"\"\"\n",
    "    pass\n",
    "\n",
    "def _hash_key(*parts: str) -> str:\n",
    "    s = \"||\".join(parts)\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _cache_read(path: str):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "def _cache_write(path: str, data):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False)\n",
    "\n",
    "def _http_request(method: str, url: str, params: Dict[str, Any] = None, json_body: Dict[str, Any] = None, timeout: int = TIMEOUT) -> requests.Response:\n",
    "    \"\"\"Maneja peticiones HTTP con reintento y captura de errores críticos.\"\"\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            if method == \"GET\":\n",
    "                r = requests.get(url, params=params, timeout=timeout)\n",
    "            elif method == \"POST\":\n",
    "                r = requests.post(url, json=json_body, timeout=timeout)\n",
    "            else:\n",
    "                raise ValueError(\"Método HTTP no soportado.\")\n",
    "            \n",
    "            # --- Manejo de errores de API ---\n",
    "            if r.status_code in [401, 403]:\n",
    "                # Error de clave o permiso: no reintentar, abortar el proceso de la API.\n",
    "                raise GoogleAPICriticalError(f\"HTTP {r.status_code}. Revisa la clave API: {r.text[:180]}\")\n",
    "            \n",
    "            if r.status_code == 200:\n",
    "                return r\n",
    "            \n",
    "            # Si es un error transitorio (5xx), reintentar\n",
    "            if r.status_code >= 500:\n",
    "                logging.warning(f\"Error HTTP {r.status_code}. Reintento {attempt+1}/{MAX_RETRIES}.\")\n",
    "                time.sleep(2 ** attempt)\n",
    "                continue\n",
    "                \n",
    "            # Otros errores no 200/4xx/5xx (raros)\n",
    "            return r\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Error de conexión, timeout, etc.\n",
    "            logging.warning(f\"Error de conexión. Reintento {attempt+1}/{MAX_RETRIES}. Error: {e}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "            \n",
    "    # Si todos los reintentos fallaron\n",
    "    raise GoogleAPIError(f\"Fallo persistente tras {MAX_RETRIES} intentos para {url}.\")\n",
    "\n",
    "# Implementaciones que usan _http_request\n",
    "def _http_get(url: str, params: Dict[str, Any] = None, timeout: int = TIMEOUT) -> requests.Response:\n",
    "    return _http_request(\"GET\", url, params=params, timeout=timeout)\n",
    "\n",
    "def _http_post(url: str, json_body: Dict[str, Any], timeout: int = TIMEOUT) -> requests.Response:\n",
    "    return _http_request(\"POST\", url, json_body=json_body, timeout=timeout)\n",
    "\n",
    "# =============== GOOGLE PLACES (Nearby Search) ===============\n",
    "def places_nearby_search(lat: float, lon: float, place_type: str, max_pages: int = MAX_PAGES) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Busca lugares cercanos para un tipo específico. Incluye paginación.\n",
    "    Lanza GoogleAPICriticalError si hay un 403/401.\n",
    "    \"\"\"\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
    "    all_results = []\n",
    "    page = 0\n",
    "    next_page_token = None\n",
    "\n",
    "    while page < max_pages:\n",
    "        params = {\n",
    "            \"key\": GOOGLE_PLACES_API_KEY,\n",
    "            \"location\": f\"{lat},{lon}\",\n",
    "            \"radius\": RADIUS_METERS,\n",
    "            \"type\": place_type\n",
    "        }\n",
    "        if next_page_token:\n",
    "            # Token de paginación no se puede combinar con otros parámetros excepto key.\n",
    "            params = {\"key\": GOOGLE_PLACES_API_KEY, \"pagetoken\": next_page_token}\n",
    "\n",
    "        r = _http_get(url, params)\n",
    "        data = r.json()\n",
    "        \n",
    "        if r.status_code != 200:\n",
    "            # Esto maneja fallos que no sean 403/401 (ya manejados en _http_request)\n",
    "            logging.warning(f\"NearbySearch JSON ERROR {r.status_code} para {place_type}: {data.get('status')}\")\n",
    "            break\n",
    "            \n",
    "        results = data.get(\"results\", [])\n",
    "        all_results.extend(results)\n",
    "        next_page_token = data.get(\"next_page_token\")\n",
    "        page += 1\n",
    "\n",
    "        if next_page_token:\n",
    "            # Espera forzada para que Google genere el token de la siguiente página\n",
    "            time.sleep(SLEEP_NEXT_PAGE) \n",
    "        else:\n",
    "            break\n",
    "\n",
    "        time.sleep(SLEEP_BETWEEN_REQS) # Espera entre diferentes peticiones\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def fetch_places_for_category(lat: float, lon: float, cat_key: str, subtypes: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Nearby Search para varios subtipos; deduplica; resume métricas.\n",
    "    \"\"\"\n",
    "    all_results: List[Dict[str, Any]] = []\n",
    "    \n",
    "    # 1. Recolección de datos por subtipo con caching\n",
    "    for subtype in subtypes:\n",
    "        # Clave de cache por coordenadas, radio, subtipo y página límite\n",
    "        cache_key = _hash_key(\"nearby\", f\"{lat:.5f}\", f\"{lon:.5f}\", str(RADIUS_METERS), subtype, str(MAX_PAGES))\n",
    "        cache_path = os.path.join(CACHE_PLACES_SEARCH, f\"{cache_key}.json\")\n",
    "        data = _cache_read(cache_path)\n",
    "        \n",
    "        if data is None:\n",
    "            data = places_nearby_search(lat, lon, subtype, MAX_PAGES)\n",
    "            _cache_write(cache_path, data)\n",
    "        \n",
    "        all_results.extend(data)\n",
    "        time.sleep(SLEEP_BETWEEN_REQS) # Evita saturar la API\n",
    "\n",
    "    # 2. Deduplicación (por place_id)\n",
    "    dedup = {}\n",
    "    for p in all_results:\n",
    "        pid = p.get(\"place_id\")\n",
    "        if not pid: continue\n",
    "        old = dedup.get(pid)\n",
    "        \n",
    "        if old is None:\n",
    "            dedup[pid] = p\n",
    "        else:\n",
    "            # Nos quedamos con el registro que tenga más reseñas\n",
    "            n_old = old.get(\"user_ratings_total\") or 0\n",
    "            n_new = p.get(\"user_ratings_total\") or 0\n",
    "            if n_new > n_old:\n",
    "                dedup[pid] = p\n",
    "\n",
    "    clean = list(dedup.values())\n",
    "\n",
    "    # 3. Métricas agregadas (rating medio ponderado por nº reseñas)\n",
    "    total_reviews = 0\n",
    "    weighted_sum = 0.0\n",
    "    for p in clean:\n",
    "        r = p.get(\"rating\")\n",
    "        n = p.get(\"user_ratings_total\") or 0\n",
    "        if r is not None and n is not None:\n",
    "            weighted_sum += r * n\n",
    "            total_reviews += n\n",
    "\n",
    "    weighted_avg = (weighted_sum / total_reviews) if total_reviews > 0 else None\n",
    "\n",
    "    return {\n",
    "        f\"{cat_key}_count\": len(clean),\n",
    "        f\"{cat_key}_reviews\": int(total_reviews),\n",
    "        f\"{cat_key}_rating_wavg\": round(weighted_avg, 3) if weighted_avg is not None else None\n",
    "    }\n",
    "\n",
    "# =============== GOOGLE AIR QUALITY (Actual) ===============\n",
    "# =============== GOOGLE AIR QUALITY (Actual) ===============\n",
    "def air_quality_current(lat: float, lon: float) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Google Air Quality API - condiciones actuales para un punto.\n",
    "    Solo devuelve campos numéricos (AQI y valores de concentración).\n",
    "    \"\"\"\n",
    "    url = f\"https://airquality.googleapis.com/v1/currentConditions:lookup?key={GOOGLE_AIR_API_KEY}\"\n",
    "    payload = {\n",
    "        \"location\": {\"latitude\": lat, \"longitude\": lon},\n",
    "        \"extraComputations\": [\"POLLUTANT_CONCENTRATION\"], # Solo necesitamos valores de concentración\n",
    "        \"languageCode\": \"es\"\n",
    "    }\n",
    "    \n",
    "    cache_key = _hash_key(\"air_current\", f\"{lat:.5f}\", f\"{lon:.5f}\")\n",
    "    cache_path = os.path.join(CACHE_AIR, f\"{cache_key}.json\")\n",
    "    data = _cache_read(cache_path)\n",
    "    \n",
    "    if data is None:\n",
    "        r = _http_post(url, payload)\n",
    "        \n",
    "        if r.status_code != 200:\n",
    "            logging.warning(f\"AirQuality JSON ERROR {r.status_code}: {r.json().get('error', {}).get('message')}\")\n",
    "            return {}\n",
    "            \n",
    "        data = r.json()\n",
    "        _cache_write(cache_path, data)\n",
    "        time.sleep(SLEEP_BETWEEN_REQS)\n",
    "\n",
    "    result = {}\n",
    "    try:\n",
    "        # 1. FIX DE ROBUSTEZ: Asegura que la data sea un diccionario antes de parsear\n",
    "        if not isinstance(data, dict):\n",
    "            logging.warning(f\"AirQuality API / Cache para {lat},{lon} devolvió un tipo inesperado: {type(data)}. Saltando parsing.\")\n",
    "            return {}\n",
    "            \n",
    "        indexes = data.get(\"indexes\", [])\n",
    "        if indexes:\n",
    "            idx = indexes[0]\n",
    "            # 2. SOLO NUMÉRICOS: Guardamos el valor numérico del AQI\n",
    "            result[\"aq_aqi\"] = idx.get(\"aqi\")\n",
    "            # Descartamos 'aq_aqi_source', 'aq_category', 'aq_dominant_pollutant'\n",
    "\n",
    "        pollutants = data.get(\"pollutants\", [])\n",
    "        for p in pollutants:\n",
    "            code = (p.get(\"code\") or p.get(\"displayName\") or \"\").lower()\n",
    "            conc = (p.get(\"concentration\") or {}).get(\"value\") # Valor de concentración (numérico)\n",
    "            # Descartamos 'unit'\n",
    "            \n",
    "            if code and conc is not None:\n",
    "                # Guardamos solo el valor numérico (e.g., aq_pm25_value)\n",
    "                result[f\"aq_{code}_value\"] = conc\n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Parse AirQuality error: {e}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# =============== PIPELINE PRINCIPAL (wide) ===============\n",
    "def process_municipio(row: pd.Series, api_status: Dict[str, bool]) -> Dict[str, Any]:\n",
    "    cod = row[COL_ID]\n",
    "    name = row[COL_NAME]\n",
    "    lat = float(row[COL_LAT])\n",
    "    lon = float(row[COL_LON])\n",
    "    pop = row.get(COL_POP, None)\n",
    "\n",
    "    logging.info(f\"Municipio: {name} ({cod})\")\n",
    "\n",
    "    rec = {\n",
    "        COL_ID: cod, COL_NAME: name, COL_LAT: lat, COL_LON: lon, COL_POP: pop\n",
    "    }\n",
    "\n",
    "    # --- Google Places (conteos + rating medio ponderado + total reseñas) ---\n",
    "    if api_status[\"places\"]:\n",
    "        for cat_key, subtypes in PLACE_TYPES.items():\n",
    "            try:\n",
    "                metrics = fetch_places_for_category(lat, lon, cat_key, subtypes)\n",
    "                rec.update(metrics)\n",
    "            except GoogleAPICriticalError as e:\n",
    "                logging.critical(f\"FATAL: {e}. Deshabilitando Google Places para el resto.\")\n",
    "                api_status[\"places\"] = False\n",
    "                # Poner Nones para esta categoría y salir del bucle de categorías\n",
    "                rec[f\"{cat_key}_count\"] = None\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error Google Places en {name} / {cat_key} (no crítico): {e}\")\n",
    "                rec[f\"{cat_key}_count\"] = None\n",
    "\n",
    "    # --- Air Quality (actual) ---\n",
    "    if api_status[\"air\"]:\n",
    "        try:\n",
    "            aq = air_quality_current(lat, lon)\n",
    "            rec.update(aq)\n",
    "        except GoogleAPICriticalError as e:\n",
    "            logging.critical(f\"FATAL: {e}. Deshabilitando Google Air Quality para el resto.\")\n",
    "            api_status[\"air\"] = False\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error AirQuality en {name} (no crítico): {e}\")\n",
    "\n",
    "    return rec\n",
    "\n",
    "def main():\n",
    "    # Validaciones de claves\n",
    "    if not GOOGLE_PLACES_API_KEY:\n",
    "        raise SystemExit(\"Falta GOOGLE_PLACES_API_KEY en .env\")\n",
    "    if not GOOGLE_AIR_API_KEY:\n",
    "        logging.warning(\"No se encontró GOOGLE_AIR_API_KEY; intentaré usar la misma de Places.\")\n",
    "\n",
    "    # Cargar CSV\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    required_cols = {COL_ID, COL_NAME, COL_LAT, COL_LON}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise SystemExit(f\"Faltan columnas en {INPUT_CSV}: {missing}\")\n",
    "\n",
    "    # Estado de las APIs (para control de errores 403/401)\n",
    "    api_status = {\"places\": True, \"air\": True}\n",
    "\n",
    "    # Procesar\n",
    "    rows = []\n",
    "    for i, row in df.iterrows():\n",
    "        logging.info(f\"[{i+1}/{len(df)}] {row[COL_NAME]}\")\n",
    "        rec = process_municipio(row, api_status)\n",
    "        rows.append(rec)\n",
    "        \n",
    "        # Guardado incremental (por si se interrumpe)\n",
    "        pd.DataFrame(rows).to_csv(OUT_WIDE, index=False)\n",
    "\n",
    "    logging.info(f\"OK. CSV wide: {OUT_WIDE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
