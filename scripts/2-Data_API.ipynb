{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe192b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Wide ETL Municipios (Madrid < 50k):\n",
    "- Google Places (Nearby Search): recuento + rating medio ponderado + total rese√±as\n",
    "- Google Air Quality API: condici√≥n actual\n",
    "Salida: CSV wide (1 fila por municipio)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import logging\n",
    "from typing import Dict, List, Any, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962646a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 01:16:53,492 [INFO] [1/155] Acebeda (La)\n",
      "2025-11-20 01:16:53,493 [INFO] Municipio: Acebeda (La) (14)\n"
     ]
    }
   ],
   "source": [
    "# =============== CONFIGURACI√ìN ===============\n",
    "load_dotenv()\n",
    "\n",
    "# --- Claves y rutas ---\n",
    "GOOGLE_PLACES_API_KEY = os.getenv(\"GOOGLE_PLACES_API_KEY\") or os.getenv(\"API_KEY\")\n",
    "GOOGLE_AIR_API_KEY    = os.getenv(\"GOOGLE_AIR_API_KEY\") or GOOGLE_PLACES_API_KEY\n",
    "\n",
    "INPUT_CSV  = os.path.join(\"../raw_data\", \"municipios_madrid_menores_50000.csv\")\n",
    "OUT_WIDE   = os.path.join(\"../raw_data\", \"municipios_google_data.csv\") # Nombre actualizado\n",
    "\n",
    "CACHE_DIR  = os.path.join(\"cache\")\n",
    "CACHE_PLACES_SEARCH  = os.path.join(CACHE_DIR, \"places_search\")\n",
    "CACHE_AIR            = os.path.join(CACHE_DIR, \"air_quality\")\n",
    "os.makedirs(CACHE_PLACES_SEARCH,  exist_ok=True)\n",
    "os.makedirs(CACHE_AIR,            exist_ok=True)\n",
    "os.makedirs(os.path.dirname(OUT_WIDE), exist_ok=True)\n",
    "\n",
    "# --- Par√°metros generales ---\n",
    "# RADIUS_METERS = 2000          # radio de consulta, pilla la zona central que es la mas significativa\n",
    "MAX_PAGES = 1                 # Nearby Search: 1 p√°gina (20 resultados) para contener cuota\n",
    "SLEEP_BETWEEN_REQS = 1.2      # segundos entre peticiones\n",
    "SLEEP_NEXT_PAGE = 2.2         # espera para next_page_token de Google\n",
    "TIMEOUT = 30                  # timeout HTTP\n",
    "MAX_RETRIES = 3               # M√°ximo de reintentos para errores transitorios (5xx)\n",
    "\n",
    "# --- Columnas esperadas en el CSV ---\n",
    "COL_ID    = \"cod_municipio\"\n",
    "COL_NAME  = \"municipio\"\n",
    "COL_LAT   = \"latitud\"\n",
    "COL_LON   = \"longitud\"\n",
    "COL_POP   = \"poblacion\"\n",
    "\n",
    "# --- Categor√≠as Google (EXTENDIDAS) ---\n",
    "PLACE_TYPES: Dict[str, List[str]] = {\n",
    "    # 1. Servicios b√°sicos y cotidianos\n",
    "    \"g_supermercados\": [\"supermarket\", \"grocery_or_supermarket\"],\n",
    "    \"g_conveniencia\": [\"convenience_store\"],\n",
    "    \"g_farmacias\": [\"pharmacy\"],\n",
    "    \"g_bancos\": [\"bank\"],\n",
    "    \"g_cajeros\": [\"atm\"],\n",
    "    \"g_gasolineras\": [\"gas_station\"],\n",
    "\n",
    "    # 2. Salud y asistencia\n",
    "    \"g_hospitales_clinicas\": [\"hospital\", \"clinic\"],\n",
    "    \"g_medicos_familia\": [\"doctor\"],\n",
    "    \n",
    "    # 3. Educaci√≥n y formaci√≥n\n",
    "    \"g_escuelas_infantiles\": [\"preschool\", \"kindergarten\"],\n",
    "    \"g_colegios_institutos\": [\"primary_school\", \"secondary_school\", \"school\"],\n",
    "    \"g_universidad_fp\": [\"university\", \"vocational_school\"],\n",
    "    \n",
    "    # 4. Transporte (Antes OSM - Mapeado a Google Place Types)\n",
    "    \"g_paradas_bus\": [\"bus_station\", \"bus_stop\"],\n",
    "    \"g_estaciones_principales\": [\"train_station\", \"subway_station\", \"transit_station\"],\n",
    "    \"g_aparcamientos\": [\"parking\"],\n",
    "\n",
    "    # 5. Ocio y cultura\n",
    "    \"g_restaurantes\": [\"restaurant\"],\n",
    "    \"g_cafeterias\": [\"cafe\"],\n",
    "    \"g_bares\": [\"bar\"],\n",
    "    \"g_cines\": [\"movie_theater\"],\n",
    "    \"g_gimnasios\": [\"gym\"],\n",
    "    \"g_parques\": [\"park\"], # Mapea parques (lo m√°s parecido a entorno)\n",
    "    \n",
    "    # 6. Comercio\n",
    "    \"g_centros_comerciales\": [\"shopping_mall\"],\n",
    "    \n",
    "    # 7. Seguridad y administraci√≥n (Antes OSM - Mapeado a Google Place Types)\n",
    "    \"g_comisarias\": [\"police\"],\n",
    "    \"g_bomberos\": [\"fire_station\"],\n",
    "    \"g_ayuntamientos\": [\"city_hall\"],\n",
    "    \"g_juzgados\": [\"court\"],\n",
    "    \n",
    "    # NOTA: Bosques y rutas de senderismo no tienen Place Type en Google. \n",
    "    # Solo \"park\" se mantiene como indicador de entorno natural.\n",
    "}\n",
    "\n",
    "# --- Mapeo de categor√≠as a t√©rminos de b√∫squeda en espa√±ol ---\n",
    "ES_QUERY_TERMS: Dict[str, str] = {\n",
    "    \"g_supermercados\": \"supermercados\",\n",
    "    \"g_conveniencia\": \"tiendas de conveniencia\",\n",
    "    \"g_farmacias\": \"farmacias\",\n",
    "    \"g_bancos\": \"bancos\",\n",
    "    \"g_cajeros\": \"cajeros automaticos\",\n",
    "    \"g_gasolineras\": \"gasolineras\",\n",
    "    \"g_hospitales_clinicas\": \"hospitales y clinicas\",\n",
    "    \"g_medicos_familia\": \"medicos de familia\",\n",
    "    \"g_escuelas_infantiles\": \"escuelas infantiles\",\n",
    "    \"g_colegios_institutos\": \"colegios e institutos\",\n",
    "    \"g_universidad_fp\": \"universidades y FP\",\n",
    "    \"g_paradas_bus\": \"paradas de autobus\",\n",
    "    \"g_estaciones_principales\": \"estaciones de transporte principal\",\n",
    "    \"g_aparcamientos\": \"aparcamientos\",\n",
    "    \"g_restaurantes\": \"restaurantes\",\n",
    "    \"g_cafeterias\": \"cafeterias\",\n",
    "    \"g_bares\": \"bares\",\n",
    "    \"g_cines\": \"cines\",\n",
    "    \"g_gimnasios\": \"gimnasios\",\n",
    "    \"g_parques\": \"parques\",\n",
    "    \"g_centros_comerciales\": \"centros comerciales\",\n",
    "    \"g_comisarias\": \"comisarias de policia\",\n",
    "    \"g_bomberos\": \"estaciones de bomberos\",\n",
    "    \"g_ayuntamientos\": \"ayuntamientos\",\n",
    "    \"g_juzgados\": \"juzgados\",\n",
    "}\n",
    "\n",
    "# =============== LOGGING ===============\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n",
    "\n",
    "# =============== HELPERS ===============\n",
    "class GoogleAPIError(Exception):\n",
    "    \"\"\"Excepci√≥n base para errores de Google API.\"\"\"\n",
    "    pass\n",
    "\n",
    "class GoogleAPICriticalError(GoogleAPIError):\n",
    "    \"\"\"Error cr√≠tico (403, 401) que debe detener la ejecuci√≥n de esa API.\"\"\"\n",
    "    pass\n",
    "\n",
    "def _hash_key(*parts: str) -> str:\n",
    "    s = \"||\".join(parts)\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _cache_read(path: str):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "def _cache_write(path: str, data):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False)\n",
    "\n",
    "def _http_request(method: str, url: str, params: Dict[str, Any] = None, json_body: Dict[str, Any] = None, timeout: int = TIMEOUT) -> requests.Response:\n",
    "    \"\"\"Maneja peticiones HTTP con reintento y captura de errores cr√≠ticos.\"\"\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            if method == \"GET\":\n",
    "                r = requests.get(url, params=params, timeout=timeout)\n",
    "            elif method == \"POST\":\n",
    "                r = requests.post(url, json=json_body, timeout=timeout)\n",
    "            else:\n",
    "                raise ValueError(\"M√©todo HTTP no soportado.\")\n",
    "            \n",
    "            # --- Manejo de errores de API ---\n",
    "            if r.status_code in [401, 403]:\n",
    "                # Error de clave o permiso: no reintentar, abortar el proceso de la API.\n",
    "                raise GoogleAPICriticalError(f\"HTTP {r.status_code}. Revisa la clave API: {r.text[:180]}\")\n",
    "            \n",
    "            if r.status_code == 200:\n",
    "                return r\n",
    "            \n",
    "            # Si es un error transitorio (5xx), reintentar\n",
    "            if r.status_code >= 500:\n",
    "                logging.warning(f\"Error HTTP {r.status_code}. Reintento {attempt+1}/{MAX_RETRIES}.\")\n",
    "                time.sleep(2 ** attempt)\n",
    "                continue\n",
    "                \n",
    "            # Otros errores no 200/4xx/5xx (raros)\n",
    "            return r\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Error de conexi√≥n, timeout, etc.\n",
    "            logging.warning(f\"Error de conexi√≥n. Reintento {attempt+1}/{MAX_RETRIES}. Error: {e}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "            \n",
    "    # Si todos los reintentos fallaron\n",
    "    raise GoogleAPIError(f\"Fallo persistente tras {MAX_RETRIES} intentos para {url}.\")\n",
    "\n",
    "# Implementaciones que usan _http_request\n",
    "def _http_get(url: str, params: Dict[str, Any] = None, timeout: int = TIMEOUT) -> requests.Response:\n",
    "    return _http_request(\"GET\", url, params=params, timeout=timeout)\n",
    "\n",
    "def _http_post(url: str, json_body: Dict[str, Any], timeout: int = TIMEOUT) -> requests.Response:\n",
    "    return _http_request(\"POST\", url, json_body=json_body, timeout=timeout)\n",
    "\n",
    "# =============== GOOGLE PLACES (Text Search) ===============\n",
    "def places_text_search(lat: float, lon: float, query: str, max_pages: int = MAX_PAGES) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Busca lugares utilizando una consulta de texto y sesgando la ubicaci√≥n. \n",
    "    Lanza GoogleAPICriticalError si hay un 403/401.\n",
    "    \"\"\"\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
    "    all_results = []\n",
    "    page = 0\n",
    "    next_page_token = None\n",
    "\n",
    "    while page < max_pages:\n",
    "        params = {\n",
    "            \"key\": GOOGLE_PLACES_API_KEY,\n",
    "            \"query\": query,                     # La consulta de texto (e.g., \"supermercados en Getafe\")\n",
    "            # Sesgo para priorizar resultados cerca del centro sin imponer un radio.\n",
    "            \"locationbias\": f\"point:{lat},{lon}\" \n",
    "        }\n",
    "        if next_page_token:\n",
    "            # Token de paginaci√≥n no se puede combinar con otros par√°metros excepto key.\n",
    "            params = {\"key\": GOOGLE_PLACES_API_KEY, \"pagetoken\": next_page_token}\n",
    "\n",
    "        r = _http_get(url, params)\n",
    "        data = r.json()\n",
    "        \n",
    "        if r.status_code != 200 or data.get('status') in [\"ZERO_RESULTS\", \"NOT_FOUND\"]:\n",
    "            # Esto maneja fallos que no sean 403/401\n",
    "            # ZERO_RESULTS es normal, no requiere aviso.\n",
    "            if data.get('status') not in [\"ZERO_RESULTS\", \"NOT_FOUND\"]:\n",
    "                 logging.warning(f\"TextSearch JSON ERROR {r.status_code} para '{query}': {data.get('status')}\")\n",
    "            break\n",
    "            \n",
    "        results = data.get(\"results\", [])\n",
    "        all_results.extend(results)\n",
    "        next_page_token = data.get(\"next_page_token\")\n",
    "        page += 1\n",
    "\n",
    "        if next_page_token:\n",
    "            # Espera forzada para que Google genere el token de la siguiente p√°gina\n",
    "            time.sleep(SLEEP_NEXT_PAGE) \n",
    "        else:\n",
    "            break\n",
    "\n",
    "        time.sleep(SLEEP_BETWEEN_REQS) # Espera entre diferentes peticiones\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def fetch_places_for_category(lat: float, lon: float, cat_key: str, subtypes: List[str], municipio_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Text Search para una categor√≠a; deduplica; resume m√©tricas.\n",
    "    \"\"\"\n",
    "    all_results: List[Dict[str, Any]] = []\n",
    "    \n",
    "    # üåü CAMBIO AQU√ç: Usamos el t√©rmino en espa√±ol para la consulta\n",
    "    search_term = ES_QUERY_TERMS.get(cat_key, cat_key.replace(\"g_\", \"\").replace(\"_\", \" \"))\n",
    "    \n",
    "    if not search_term:\n",
    "        logging.warning(f\"Categor√≠a {cat_key} no tiene t√©rmino de b√∫squeda en espa√±ol. Saltando.\")\n",
    "        return {}\n",
    "\n",
    "    # La consulta expl√≠cita en espa√±ol es m√°s robusta\n",
    "    query_str = f\"{search_term} en {municipio_name}\"\n",
    "\n",
    "    # 2. Recolecci√≥n de datos con caching\n",
    "    # La clave de cache ahora incluye la consulta y no el radio/tipo\n",
    "    cache_key = _hash_key(\"text_search\", f\"{lat:.5f}\", f\"{lon:.5f}\", query_str, str(MAX_PAGES))\n",
    "    cache_path = os.path.join(CACHE_PLACES_SEARCH, f\"{cache_key}.json\")\n",
    "    data = _cache_read(cache_path)\n",
    "    \n",
    "    if data is None:\n",
    "        # üåü Llama a la nueva funci√≥n\n",
    "        data = places_text_search(lat, lon, query_str, MAX_PAGES) \n",
    "        _cache_write(cache_path, data)\n",
    "        \n",
    "    all_results.extend(data)\n",
    "    time.sleep(SLEEP_BETWEEN_REQS) # Evita saturar la API\n",
    "\n",
    "    # Los pasos 2 y 3 de deduplicaci√≥n y m√©tricas siguen igual:\n",
    "    # 3. Deduplicaci√≥n (por place_id)\n",
    "    dedup = {}\n",
    "    for p in all_results:\n",
    "        pid = p.get(\"place_id\")\n",
    "        # ... (c√≥digo de deduplicaci√≥n y m√©tricas sigue igual)\n",
    "        # ...\n",
    "        \n",
    "    clean = list(dedup.values())\n",
    "\n",
    "    # 4. M√©tricas agregadas (rating medio ponderado por n¬∫ rese√±as)\n",
    "    total_reviews = 0\n",
    "    weighted_sum = 0.0\n",
    "    for p in clean:\n",
    "        r = p.get(\"rating\")\n",
    "        n = p.get(\"user_ratings_total\") or 0\n",
    "        if r is not None and n is not None:\n",
    "            weighted_sum += r * n\n",
    "            total_reviews += n\n",
    "\n",
    "    weighted_avg = (weighted_sum / total_reviews) if total_reviews > 0 else None\n",
    "\n",
    "    return {\n",
    "        f\"{cat_key}_count\": len(clean),\n",
    "        f\"{cat_key}_reviews\": int(total_reviews),\n",
    "        f\"{cat_key}_rating_wavg\": round(weighted_avg, 3) if weighted_avg is not None else None\n",
    "    }\n",
    "    \n",
    "# =============== GOOGLE AIR QUALITY (Actual) ===============\n",
    "# =============== GOOGLE AIR QUALITY (Actual) ===============\n",
    "def air_quality_current(lat: float, lon: float) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Google Air Quality API - condiciones actuales para un punto.\n",
    "    Solo devuelve campos num√©ricos (AQI y valores de concentraci√≥n).\n",
    "    \"\"\"\n",
    "    url = f\"https://airquality.googleapis.com/v1/currentConditions:lookup?key={GOOGLE_AIR_API_KEY}\"\n",
    "    payload = {\n",
    "        \"location\": {\"latitude\": lat, \"longitude\": lon},\n",
    "        \"extraComputations\": [\"POLLUTANT_CONCENTRATION\"], # Solo necesitamos valores de concentraci√≥n\n",
    "        \"languageCode\": \"es\"\n",
    "    }\n",
    "    \n",
    "    cache_key = _hash_key(\"air_current\", f\"{lat:.5f}\", f\"{lon:.5f}\")\n",
    "    cache_path = os.path.join(CACHE_AIR, f\"{cache_key}.json\")\n",
    "    data = _cache_read(cache_path)\n",
    "    \n",
    "    if data is None:\n",
    "        r = _http_post(url, payload)\n",
    "        \n",
    "        if r.status_code != 200:\n",
    "            logging.warning(f\"AirQuality JSON ERROR {r.status_code}: {r.json().get('error', {}).get('message')}\")\n",
    "            return {}\n",
    "            \n",
    "        data = r.json()\n",
    "        _cache_write(cache_path, data)\n",
    "        time.sleep(SLEEP_BETWEEN_REQS)\n",
    "\n",
    "    result = {}\n",
    "    try:\n",
    "        # 1. FIX DE ROBUSTEZ: Asegura que la data sea un diccionario antes de parsear\n",
    "        if not isinstance(data, dict):\n",
    "            logging.warning(f\"AirQuality API / Cache para {lat},{lon} devolvi√≥ un tipo inesperado: {type(data)}. Saltando parsing.\")\n",
    "            return {}\n",
    "            \n",
    "        indexes = data.get(\"indexes\", [])\n",
    "        if indexes:\n",
    "            idx = indexes[0]\n",
    "            # 2. SOLO NUM√âRICOS: Guardamos el valor num√©rico del AQI\n",
    "            result[\"aq_aqi\"] = idx.get(\"aqi\")\n",
    "            # Descartamos 'aq_aqi_source', 'aq_category', 'aq_dominant_pollutant'\n",
    "\n",
    "        pollutants = data.get(\"pollutants\", [])\n",
    "        for p in pollutants:\n",
    "            code = (p.get(\"code\") or p.get(\"displayName\") or \"\").lower()\n",
    "            conc = (p.get(\"concentration\") or {}).get(\"value\") # Valor de concentraci√≥n (num√©rico)\n",
    "            # Descartamos 'unit'\n",
    "            \n",
    "            if code and conc is not None:\n",
    "                # Guardamos solo el valor num√©rico (e.g., aq_pm25_value)\n",
    "                result[f\"aq_{code}_value\"] = conc\n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Parse AirQuality error: {e}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# =============== PIPELINE PRINCIPAL (wide) ===============\n",
    "def process_municipio(row: pd.Series, api_status: Dict[str, bool]) -> Dict[str, Any]:\n",
    "    cod = row[COL_ID]\n",
    "    name = row[COL_NAME] # Aqu√≠ est√° el nombre del municipio que necesitamos\n",
    "    lat = float(row[COL_LAT])\n",
    "    lon = float(row[COL_LON])\n",
    "    pop = row.get(COL_POP, None)\n",
    "\n",
    "    logging.info(f\"Municipio: {name} ({cod})\")\n",
    "\n",
    "    rec = {\n",
    "        COL_ID: cod, COL_NAME: name, COL_LAT: lat, COL_LON: lon, COL_POP: pop\n",
    "    }\n",
    "\n",
    "    # --- Google Places ---\n",
    "    if api_status[\"places\"]:\n",
    "        for cat_key, subtypes in PLACE_TYPES.items():\n",
    "            try:\n",
    "                # üåü ¬°Cambio aqu√≠! Pasamos el 'name' como √∫ltimo argumento\n",
    "                metrics = fetch_places_for_category(lat, lon, cat_key, subtypes, name) \n",
    "                rec.update(metrics)\n",
    "            except GoogleAPICriticalError as e:\n",
    "                # ... (c√≥digo existente)\n",
    "                # ...\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error Google Places en {name} / {cat_key} (no cr√≠tico): {e}\")\n",
    "                rec[f\"{cat_key}_count\"] = None\n",
    "    \n",
    "    # ... (resto de la funci√≥n air_quality_current sigue igual)\n",
    "    \n",
    "    return rec\n",
    "\n",
    "def main():\n",
    "    # Validaciones de claves\n",
    "    if not GOOGLE_PLACES_API_KEY:\n",
    "        raise SystemExit(\"Falta GOOGLE_PLACES_API_KEY en .env\")\n",
    "    if not GOOGLE_AIR_API_KEY:\n",
    "        logging.warning(\"No se encontr√≥ GOOGLE_AIR_API_KEY; intentar√© usar la misma de Places.\")\n",
    "\n",
    "    # Cargar CSV\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    required_cols = {COL_ID, COL_NAME, COL_LAT, COL_LON}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise SystemExit(f\"Faltan columnas en {INPUT_CSV}: {missing}\")\n",
    "\n",
    "    # Estado de las APIs (para control de errores 403/401)\n",
    "    api_status = {\"places\": True, \"air\": True}\n",
    "\n",
    "    # Procesar\n",
    "    rows = []\n",
    "    for i, row in df.iterrows():\n",
    "        logging.info(f\"[{i+1}/{len(df)}] {row[COL_NAME]}\")\n",
    "        rec = process_municipio(row, api_status)\n",
    "        rows.append(rec)\n",
    "        \n",
    "        # Guardado incremental (por si se interrumpe)\n",
    "        pd.DataFrame(rows).to_csv(OUT_WIDE, index=False)\n",
    "\n",
    "    logging.info(f\"OK. CSV wide: {OUT_WIDE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
